{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bogdan Bintu\n",
    "### Copyright Presidents and Fellows of Harvard College, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard analysis of the STORM data\n",
    "\n",
    "The subfolders H#R# indicate the hybrdization round and the index of the 30kb region imaged respectively.\n",
    "\n",
    "The region chr21:28Mb-29.2Mb was imaged with 30Kb resolution using STORM across two independent datasets with 100-150 chromosomes each.\n",
    "\n",
    "Each subfolder contains both diffraction-limited and STORM imaged fields of view, each corresponding to a '.dax' file with the fluorescent signal recorded by the camera. \n",
    "This file format can be opened with tools in ChromatinImaging\\CommonTools\\IOTools.py\n",
    "\n",
    "#### Imaging experimental details:\n",
    "\n",
    "For the first round of hybridization in a multiplexed STORM imaging experiment we mixed into the hybridization buffer the following oligonucleotides: 1) the first 30-nt Alexa647-labeled readout probe (at 100 nM) which has homology to the readout sequence on the primary probes targeting the first 30 kb of chromatin (chr21:28000001-28030000) and 2) an Alexa405-labeled activator probe (at 100 nM).\n",
    "\n",
    "For the subsequent rounds we added: 1) an Alexa647-labeled readout probe (100 nM) targeting the readout sequence on the primary probes hybridized to the next 30-kb segment and 2) an unlabeled oligo sequence (1 Î¼M) fully complementary to the previously flowed readout probe. The unlabeled oligo is used to remove the previous readout probe as detailed in the toehold displacement reaction.\n",
    "\n",
    "We used the first 41 rounds of sequential hybridization and imaging of readout probes to target the 41 consecutive 30-kb chromatin segments in the genomic region of interest. After this, we used additional rounds to relabel some of the readout probes for assessing alignment accuracy and sample variation over time. We performed three-dimensional difraction-limited and then STORM imaging after each round of hybridization to each chromatin segment. \n",
    "\n",
    "Imaging of ~18 fields of view for the 41 segments took approximately 5 days in total. \n",
    "\n",
    "The 3-valve fluidics system that we constructed allowed for the loading of only 20 different hybridization solutions. Consequently, after every 20 rounds we short circuited the fluidics chamber, washed each of the 20 channels with 30% formamide in water and loaded new hybridization solutions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script used for STORM analysis\n",
    "#### Each notebook sections should be performed sequentially. Note: some parts require GUI input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#External imports\n",
    "import numpy as np\n",
    "import os\n",
    "#Basic functions are located here:\n",
    "import GeneralToolsSTORMTracing as gtct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Get a list of all .dax files corresponding to STORM movies residing in the first hybe folder.\n",
    "### Start a series of GUIs which allow selecting the chromosomal positions based on the Cy3 signal in the first hybe of the STORM movies.\n",
    "\n",
    "### Naming convention:\n",
    "### A master folder contains subfolders labelled H#iR#j.  (hybe round #i, region #j)\n",
    "### The subfolders contain matching STORM and difraction-limited imaging files labelled STORM_#k.dax and Conv_zscan_#k.dax where #k indicates the field of view number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Part 1\n",
    "##Get a list of all .dax files corresponding to STORM movies residing in the first hybe folder.\n",
    "##Start a series of GUIs which allow selecting the chromosomal positions based on the Cy3 signal in the first hybe of the STORM movies.\n",
    "\n",
    "first_hybe_folder = r\"\"\n",
    "list_daxes = gtct.listDax(first_hybe_folder,name='STORM_')\n",
    "dic_dax = [gtct.STORM_STD_pointSelect(dax_fl) for dax_fl in list_daxes[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Decide on a master_save_folder and save the selected positions\n",
    "master_save_folder = r\"\"\n",
    "save_folder = master_save_folder+os.sep+'selectedSpots'\n",
    "if not os.path.exists(save_folder): os.makedirs(save_folder)\n",
    "big_dic={}\n",
    "for dic in dic_dax:\n",
    "    big_dic.update(dic)\n",
    "for dax_fl in big_dic.keys():\n",
    "    imshow_obj = big_dic[dax_fl]\n",
    "    #save coords\n",
    "    dic_name = gtct.save_name_dic(dax_fl,save_folder=master_save_folder)\n",
    "    pickle.dump(imshow_obj.coords,open(dic_name[\"coords_file\"],'w'))\n",
    "    #save figure\n",
    "    imshow_obj.fig.set_size_inches(12, 12)\n",
    "    imshow_obj.fig.savefig(dic_name[\"png_figure\"], dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "#### Once a round of hybridization and imaging is complete the corresponding subfolder H#iR#j folder is copied automatically from the solid state drive to one of 3 8Tb drives available.\n",
    "#### Get pointers to these files and start aligning them to pixel precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drive_folders = [r'drive1_folder',\n",
    "                 r'drive2_folder',\n",
    "                 r'drive3_folder']\n",
    "#List all the dax\n",
    "dax_files=[]\n",
    "for data_folder in drive_folders:\n",
    "    dax_files += gtct.listDax(data_folder,name='STORM_')\n",
    "#Next partition according to field of view and sort according to the order of hybridization\n",
    "fovs = [gtct.extract_flag(dax_file,'STORM_','.') for dax_file in dax_files]\n",
    "def sort_to_hybe(dax_fls):\n",
    "    r'given a list of dax files containing the hybe folder info \\H<index>R...\\it returns an sorted by index list of dax files'\n",
    "    map_=[int(dax_fl.split(os.sep+'H')[-1].split('R')[0]) for dax_fl in dax_fls]\n",
    "    return list(np.array(dax_fls)[np.argsort(map_)])\n",
    "dax_files_fovs = map(sort_to_hybe,gtct.partition_map(dax_files,fovs))\n",
    "\n",
    "#save file names for convenience in a subfolder called roughAlingment\n",
    "save_folder = master_save_folder+os.sep+'roughAlingment'\n",
    "if not os.path.exists(save_folder): os.makedirs(save_folder)\n",
    "pickle.dump(dax_files_fovs,open(save_folder+os.sep+'dax_files_fovs.pkl','w'))\n",
    "\n",
    "## Perform rough registration and save results.\n",
    "ind_ref = 0 #The hybe index to align to. By default everything is aligned to the first hybe.\n",
    "dic_registration={} \n",
    "coords_dic={}\n",
    "refs_dic={}\n",
    "\n",
    "# If failed or decide to restart load previously saved results.\n",
    "dic_registration_fl = save_folder+os.sep+'dic_registration.pkl'\n",
    "if os.path.exists(dic_registration_fl):\n",
    "    dic_registration=pickle.load(open(dic_registration_fl,'r'))\n",
    "    coords_dic=pickle.load(open(save_folder+os.sep+'coords_dic.pkl','r'))\n",
    "    refs_dic=pickle.load(open(save_folder+os.sep+'refs_dic.pkl','r'))\n",
    "\n",
    "for cy3_dax_files_fov in dax_files_fovs:\n",
    "    nm0 = cy3_dax_files_fov[ind_ref]\n",
    "    im0 = gtct.load_cy3(nm0,func=np.max)\n",
    "    for nm1 in cy3_dax_files_fov[:]:\n",
    "        basenm0 = \"--\".join(nm0.split(os.sep)[-2:])\n",
    "        basenm1 = \"--\".join(nm1.split(os.sep)[-2:])\n",
    "        key_nm = basenm0+'<-'+basenm1\n",
    "        print key_nm\n",
    "        #if not dic_registration.has_key(key_nm):\n",
    "        if not coords_dic.has_key(nm1):\n",
    "            im1 = gtct.load_cy3(nm1,func=np.max)\n",
    "            txy = gtct.fftalign_guess(im0,im1,center=[0,0],max_disp=50) # this is the main function.  Register everyone to the ref field of view using fft correlation.\n",
    "            dic_registration[key_nm] = txy # collect to dictionary\n",
    "\n",
    "            coord_file = gtct.save_name_dic(nm0,master_save_folder)[\"coords_file\"]\n",
    "            if os.path.exists(coord_file):\n",
    "                coords = pickle.load(open(coord_file,'r'))\n",
    "                coords_dic[nm1]=np.array(coords)-np.array([txy[::-1]])\n",
    "                refs_dic[nm1] = np.array(coords)\n",
    "            else:\n",
    "                coords_dic[nm1] = coords_dic[nm0]-np.array([txy[::-1]])\n",
    "                refs_dic[nm1] = refs_dic[nm0]\n",
    "            #save figure\n",
    "            fig = plt.figure(figsize=(12,12))\n",
    "            ax = fig.add_subplot(111)\n",
    "            xy = zip(*coords_dic[nm1])\n",
    "            if len(xy)>0:\n",
    "                ax.plot(np.array(xy[0])-1,np.array(xy[1])-1, 'o',markersize=12,markeredgewidth=1,markeredgecolor='g',markerfacecolor='None')\n",
    "                for i_txt,(x_,y_) in enumerate(coords_dic[nm1]):\n",
    "                    ax.text(x_-1,y_-1,str(i_txt),color='g')\n",
    "            ax.imshow(im1,cmap=cm.hot,interpolation='nearest')\n",
    "\n",
    "            file_ = \"--\".join(nm1.split(os.sep)[-2:][::-1]).replace('.dax','')+'_cy3-roughalignment.png'\n",
    "            fig.savefig(save_folder+os.sep+file_)\n",
    "            plt.close(fig)\n",
    "            #save dics\n",
    "            pickle.dump(coords_dic,open(save_folder+os.sep+'coords_dic.pkl','w'))\n",
    "            pickle.dump(refs_dic,open(save_folder+os.sep+'refs_dic.pkl','w'))\n",
    "            pickle.dump(dic_registration,open(dic_registration_fl,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### Crop data to selected chromosomes\n",
    "### Note: The cropping is very slow even with memorymapping. Hard limitation due to the nature of HDD. Croping from the SSD is faster but SSD drives of large size are expensive. Automatic croping during aquisition would be more elegant. However it is more risky due to possible registration errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load relevant data\n",
    "master_save_folder = r''\n",
    "save_folder = master_save_folder+os.sep+'roughAlingment'\n",
    "# load list of all storm dax files grouped according to field of view\n",
    "dax_files_fovs = pickle.load(open(save_folder+os.sep+'dax_files_fovs.pkl','r'))\n",
    "# load the registration dictionaries\n",
    "dic_registration = pickle.load(open(save_folder+os.sep+'dic_registration.pkl','r'))\n",
    "coords_dic = pickle.load(open(save_folder+os.sep+'coords_dic.pkl','r'))\n",
    "refs_dic = pickle.load(open(save_folder+os.sep+'refs_dic.pkl','r'))\n",
    "\n",
    "save_folder = master_save_folder+os.sep+'crop'\n",
    "\n",
    "#Partition according to drive and start cropping.\n",
    "dax_files_fovs_drive = gtct.partition_map(gtct.flatten(dax_files_fovs),[fl[0] for fl in gtct.flatten(dax_files_fovs)])\n",
    "\n",
    "## Multiple hard drives can be cropped at the same time. \n",
    "## Durring aquisition folders are copied distributively across separete hard drives. Thus the cropping from multiple hard drives could be started in parallel.\n",
    "index_drive = 0\n",
    "dax_files = dax_files_fovs_drive[index_drive]\n",
    "def sort_val(fl):\n",
    "    hindex_2digits = '{:0>2}'.format(fl.split(os.sep+'H')[-1].split('R')[0])\n",
    "    fov_index = fl.split(os.sep+'STORM_')[-1].split('.dax')[0]\n",
    "    return hindex_2digits+'_'+fov_index\n",
    "    \n",
    "ind_sort = np.argsort(map(sort_val,dax_files))\n",
    "dax_files = list(np.array(dax_files)[ind_sort])\n",
    "\n",
    "failed=[]\n",
    "\n",
    "for dax_fl in dax_files:\n",
    "    print \"Dealing with: \"+str(dax_fl)\n",
    "    coords_ = np.array(coords_dic[dax_fl],dtype=int)\n",
    "    refs_ = np.array(refs_dic[dax_fl],dtype=int)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        gtct.STORM_STD_crop(dax_fl,coords_,save_folder=save_folder,s_good=50,rep_fr = 50,memmap=False,tag=map(str,map(list,refs_)),overwrite=False)\n",
    "    except:\n",
    "        failed.append(dax_fl)\n",
    "    print \"Elapsed time: \"+str(time.time()-start)\n",
    "print \"Failed cropping files:\"\n",
    "print failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "#### Copy conventional imaging files to local save folder. These are small compared to the STORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_save_folder = r\"\"\n",
    "save_folder = master_save_folder+os.sep+'roughAlingment'\n",
    "dax_files_fovs = pickle.load(open(save_folder+os.sep+'dax_files_fovs.pkl','r'))\n",
    "\n",
    "folders = np.unique([os.path.dirname(fl) for fl in gtct.flatten(dax_files_fovs)])\n",
    "list_all_files=[]\n",
    "for folder in folders:\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for nm in files:\n",
    "            list_all_files.append(os.path.join(root, nm))\n",
    "list_files = np.setdiff1d(list_all_files,gtct.flatten(dax_files_fovs)) #exclude the STORM files\n",
    "sizes = [os.path.getsize(fl) for fl in list_files]\n",
    "print \"Size of conventional files: \" + str(np.sum(sizes)/10.**9) + \" GB\"\n",
    "\n",
    "#prepare folders for copying\n",
    "master_conv_folder = master_save_folder+os.sep+'ConvEmptyStorm'\n",
    "files_i = list_files[:]\n",
    "files_f = [master_conv_folder+os.sep+os.sep.join(fl.split(os.sep)[-2:]) for fl in files_i]\n",
    "for fl in files_f:\n",
    "    folder = os.path.dirname(fl)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "files_i_cp,files_f_cp=[],[]\n",
    "for fl1,fl2 in zip(files_i,files_f):\n",
    "    if not os.path.exists(fl2):\n",
    "        files_i_cp.append(fl1)\n",
    "        files_f_cp.append(fl2)\n",
    "\n",
    "#copy!\n",
    "import copyFilesWindows as cfw      \n",
    "cfw.copy(files_i_cp,files_f_cp)#This could be replaced with shutil copying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "#### Start batch fitting the cropped STORM files on multiple processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import StormAnalysisAdditions as saa\n",
    "\n",
    "master_save_folder = r\"\"\n",
    "save_folder = master_save_folder+os.sep+'roughAlingment'\n",
    "dax_files = glob.glob(r'F:\\Bogdan_Analysis\\10_25_2017_CDSH30k-rep5\\crop'+os.sep+'*_storm.dax')\n",
    "def sort_by_fov_and_hybe(fl):\n",
    "    hindex_2digits = '{:0>2}'.format(fl.split('--H')[-1].split('R')[0].split('A')[0])\n",
    "    fov_index = fl.split('--STORM_')[-1].split('_')[0]\n",
    "    return fov_index+'_'+hindex_2digits\n",
    "ind_sort = np.argsort(map(sort_by_fov_and_hybe,dax_files))\n",
    "dax_files = list(np.array(dax_files)[ind_sort])\n",
    "dax_files = [fl for fl in dax_files if not os.path.exists(fl.replace('.dax','_mlist.bin'))] #do not overwrite\n",
    "bin_files = [fl.replace('.dax','_mlist.bin') for fl in dax_files]\n",
    "parm_file = r'fit3DStorm.xml'#this is the paramater file used for fitting. \n",
    "#More details on this in the ZhuangLab/storm_analysis git repository\n",
    "#The z paramaters match the PSF of the STORM setup\n",
    "parms_files = [parm_file for fl in dax_files]\n",
    "saa.fitFrameBatch(dax_files, bin_files, parms_files, over_write=1, mock=False, batch_size_=50)\n",
    "#Note: This batch fitting is performed on a 20-core CPU machine with 256Gb of RAM. Adjust the batch_size acording to your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
